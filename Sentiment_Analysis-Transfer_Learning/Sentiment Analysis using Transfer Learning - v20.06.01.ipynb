{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:block\">\n",
    "    <div style=\"width: 10%; display: inline-block; text-align: left;\">\n",
    "    </div>\n",
    "    <div style=\"width: 69%; display: inline-block\">\n",
    "        <h5  style=\"color:maroon; text-align: center; font-size:25px;\">Sentiment Analysis using Transfer Learning</h5>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Sentiment Classification is a perfect problem in Natural language Processing (NLP) for getting started in it. As the name suggests, it is classification of peoples opinion or expressions into different sentiments, such as __Positive__, __Neutral__, and __Negative__.\n",
    "\n",
    "NLP is a powerful tool, but in real-world we often come across tasks which suffer from data deficit and poor model generalisation. __Transfer learning__ solved this problem. It is the process of training a model on a large-scale dataset and then using that pretrained model to conduct learning for another downstream task (i.e., target task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:33.554769Z",
     "start_time": "2020-06-11T07:24:30.243985Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# basic data processing\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for EDA\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# for text preprocessing\n",
    "import re\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# progress bar\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# instantiate\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "# for wordcloud\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for aesthetics and plots\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from termcolor import colored\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# for model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "display(Markdown('_All libraries are imported successfully!_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Preprocessing\n",
    "\n",
    "In this notebook, I am using __[Sentiment140](http://help.sentiment140.com/for-students)__. It contains two labeled data:\n",
    "* data of __1.6 Million Tweets__ to be used as __train,validation,test split data__\n",
    "* data of __498 Tweets__ to be used as another fresh __test data__\n",
    "\n",
    "Data dictionary are as follows:\n",
    "\n",
    "* __target__: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "* __ids__: The id of the tweet (2087)\n",
    "* __date__: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "* __flag__: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "* __user__: the user that tweeted (robotickilldozr)\n",
    "* __text__: the text of the tweet (Lyx is cool)\n",
    "\n",
    "__NOTE__: The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:25:40.395604Z",
     "start_time": "2020-06-11T07:25:37.185813Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names =  ['target', 'id', 'date', 'flag','user','text']\n",
    "\n",
    "df = pd.read_csv('./data/sentiment140data/training_data.csv', encoding = \"ISO-8859-1\", names=col_names)\n",
    "\n",
    "print(colored('DATA','blue',attrs=['bold']))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the data for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T05:03:39.260755Z",
     "start_time": "2020-06-10T05:01:46.236265Z"
    }
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be requiring only __target__ and __text__ columns. As observed from the above report, we have only __positive (4)__ and __negative (0)__ sentiment. We will replace 4 as 1 for convenience. \n",
    "\n",
    "Also, it's a __perfectly balanced__ dataset without any skewness - equal distribution of positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:25:47.470417Z",
     "start_time": "2020-06-11T07:25:47.323391Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "df.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)\n",
    "\n",
    "# replacing positive sentiment 4 with 1\n",
    "df.target = df.target.replace(4,1)\n",
    "\n",
    "target_count = df.target.value_counts()\n",
    "\n",
    "category_counts = len(target_count)\n",
    "display(Markdown('__Number of categories__: {}'.format(category_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "At first glance, it's evident that the data is not clean. Tweet texts often consists of other user mentions, hyperlink texts, emoticons and special characters which no value as feature to the model we are training. So we need to get rid of them. To do this, we need to perform 4 crucial process step-by-step:\n",
    "\n",
    "1. __Hyperlinks and Mentions__: In Twitter, people can tag/mention other people's ID and share URLs/hyperlinks. We need to eliminate this as well.\n",
    "\n",
    "2. __Stopwords__ : These are commonly used words (such as “the”, “a”, “an”, “in”) which have no contextual meaning in a sentence and hence we ignore them when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "3. __Spelling Correction__: We can definitely expect incorrect spellings in the tweets/data, and we need to fix as many as possible, because without doing this, the following step will not work properly.\n",
    "\n",
    "4. __Stemming/Lemmatization__: The goal of both stemming and lemmatization is to reduce inflectional and derivationally related forms of a word to a common base form. However, there is a difference which you can understand from the image below.\n",
    "\n",
    "![](./source/stem_lemm.png)\n",
    "\n",
    "Lemmatization is similar to stemming with one difference - the final form is also a __meaningful word__. Thus, stemming operation does not need a dictionary like lemmatization. Hence, here we will be going ahead with lemmetization.\n",
    "\n",
    "1, 2 and 4 can be done using the library __`NLTK`__, and spell-checking using __`pyspellchecker`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:55.043859Z",
     "start_time": "2020-06-11T07:24:55.020965Z"
    }
   },
   "outputs": [],
   "source": [
    "# set of stop words declared\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "display(Markdown('__List of stop words__:'))\n",
    "display(Markdown(str(stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words like `not`, `haven't`, `don't` are included in stopwords and ignoring them will make sentences like `this was not good` and `this was good` or `He is a nice guy... not!` and `He is a nice guy... !` have same predictions. So we need to eliminate the words that expresses negation, denial, refusal or prohibition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:24:57.700563Z",
     "start_time": "2020-06-11T07:24:57.684710Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_stop_words = stop_words.copy()\n",
    "for word in stop_words:\n",
    "    if \"n't\" in word or \"no\" in word or word.endswith('dn') or word.endswith('sn') or word.endswith('tn'):\n",
    "        updated_stop_words.remove(word)\n",
    "\n",
    "# custom select words you don't want to eliminate\n",
    "words_to_remove = ['for','by','with','against','shan','don','aren','haven','weren','until','ain','but','off','out']\n",
    "for word in words_to_remove:\n",
    "    updated_stop_words.remove(word)\n",
    "\n",
    "display(Markdown('__Updated list of stop words__:'))\n",
    "display(Markdown(str(updated_stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the function to perform the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:32:05.113716Z",
     "start_time": "2020-06-11T07:32:04.993186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "# Defining regex patterns.\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "# creating instance of spellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# creating instance of lemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preproc ess(tweet):\n",
    "    # lowercase the tweets\n",
    "    tweet = tweet.lower().strip()\n",
    "    \n",
    "    # REMOVE all URls\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    \n",
    "    # Replace all emojis.\n",
    "    for emoji in emojis.keys():\n",
    "        tweet = tweet.replace(emoji, \"emoji\" + emojis[emoji])        \n",
    "    \n",
    "    # Remove @USERNAME\n",
    "    tweet = re.sub(userPattern,'', tweet)        \n",
    "    \n",
    "    # Replace all non alphabets.\n",
    "    tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    tokenized_tweet = tweet.split()\n",
    "    \n",
    "#     # spell checks\n",
    "#     misspelled = spell.unknown(tokenized_tweet)\n",
    "#     if misspelled == set():\n",
    "#         pass\n",
    "#     else:\n",
    "#         for i,word in enumerate(misspelled):\n",
    "#             tokenized_tweet[i] = spell.correction(word)\n",
    "\n",
    "    tweetwords = ''\n",
    "    for word in tokenized_tweet:\n",
    "        # Checking if the word is a stopword.\n",
    "        if word not in updated_stop_words:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                lem_word = lemm.lemmatize(word)\n",
    "                tweetwords += (lem_word+' ')\n",
    "    \n",
    "    return tweetwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the function _`preprocess`_ on each value of the column `text` where tweets are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:42:27.280987Z",
     "start_time": "2020-06-11T07:40:57.227211Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].progress_apply(lambda x: preprocess(x))\n",
    "print(colored('DATA','blue',attrs=['bold']))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the words that are frequently used for positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:29:27.533054Z",
     "start_time": "2020-06-09T13:29:27.521057Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_wordcloud(text, mask, title = None):\n",
    "    wordcloud = WordCloud(background_color='black', max_words = 200,\n",
    "                          max_font_size = 200, random_state = 42, mask = mask)\n",
    "    wordcloud.generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(25,25))\n",
    "    \n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title, fontdict={'size': 40, 'verticalalignment': 'bottom'})\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:30:15.616197Z",
     "start_time": "2020-06-09T13:29:27.537809Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df[df['target'] == 1]['text'])\n",
    "pos_mask = np.array(Image.open('./source/upvote.png'))\n",
    "\n",
    "plot_wordcloud(pos_text, pos_mask, title = 'Most common 200 words in positive tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:30:50.784180Z",
     "start_time": "2020-06-09T13:30:15.618012Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neg_text = \" \".join(df[df['target'] == 0]['text'])\n",
    "neg_mask = np.array(Image.open('./source/downvote.png'))\n",
    "\n",
    "plot_wordcloud(neg_text, neg_mask,\n",
    "               title = 'Most common 200 words in negative tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "\n",
    "We will shuffle the dataset and split it to gives __train__, __validation__ and __test__ dataset. It's important to shuffle our dataset before training. The split is in the ratio of __6:2:2__ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T11:21:13.624076Z",
     "start_time": "2020-06-10T11:21:12.804024Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=369):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test\n",
    "\n",
    "train_df, val_df, test_df = train_validate_test_split(df)\n",
    "\n",
    "print('Train: {}, Validation: {}, Test: {}'.format(train_df.shape, val_df.shape, test_df.shape))\n",
    "\n",
    "print(colored('TRAIN DATA','magenta',attrs=['bold']))\n",
    "display(train_df.head())\n",
    "\n",
    "train_text = train_df['text'].tolist()\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = np.asarray(pd.get_dummies(train_df['target']), dtype = np.int8)\n",
    "\n",
    "val_text = val_df['text'].tolist()\n",
    "val_text = np.array(val_text, dtype=object)[:, np.newaxis]\n",
    "val_label = np.asarray(pd.get_dummies(val_df['target']), dtype = np.int8)\n",
    "\n",
    "test_text = test_df['text'].tolist()\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = np.asarray(pd.get_dummies(test_df['target']), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Embedding model\n",
    "\n",
    "## Understanding the difference\n",
    "\n",
    "There are two types of embedding in NLP domain:\n",
    "\n",
    "__WORD EMBEDDING__\n",
    "\n",
    "* Baseline\n",
    "    1. __Word2Vec__\n",
    "    2. __GloVe__\n",
    "    3. __FastText__\n",
    "* State-of-the-art\n",
    "    1. __ELMo__ (`E`mbeddeding from `L`anguage `Mo`del)\n",
    "    2. __BERT__ (`B`idirectional `E`ncoder `R`epresentations from `T`ransformers)\n",
    "    3. __OpenAI GPT__ (`G`enerative `P`re-Training `T`ransformer)\n",
    "    4. __ULMFiT__ (`U`niversal `L`anguage `M`odel `Fi`ne-`T`uning) - This is more of a process that includes word embedding along with NN architecture.\n",
    "\n",
    "__SENTENCE EMBEDDING__\n",
    "\n",
    "* Basline\n",
    "    1. __Bag of Words__\n",
    "    2. __Doc2Vec__\n",
    "* State-of-the-art\n",
    "    1. __Sentence BERT__\n",
    "    2. __Skip-Thoughts and Quick-Thoughts__\n",
    "    2. __InferSent__\n",
    "    3. __Universal Sentence Encoder__\n",
    "\n",
    "So, the fundamental difference is that __Word Embedding__ turns a word to N-dimensional vector, but the __Sentence Embedding__ is much more powerful because it is able to embed not only words but phrases and sentences as well.\n",
    "\n",
    "__ULMFiT__ is considered to be the best choice for Transfer Learning in NLP but it is built using __fast.ai__ library in which the code implementation is different from that of __Keras__ or __Tensorflow__. For this notebook, we will be using __Universal Sentence Encoder__.\n",
    "\n",
    "## Universal Sentence Encoder\n",
    "\n",
    "It can be used for text classification, semantic similarity, clustering and other natural language tasks. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs.\n",
    "\n",
    "It takes __variable length English text as input__ and __outputs a 512-dimensional vector__. Handling variable length text input sounds great, but the problem is that as sentence keeps getting longer counted by words, the more diluted embedding results could be.\n",
    "\n",
    "Hence, there are 2 Universal Sentence Encoders to choose from with different encoder architectures to achieve distinct design goals:\n",
    "* __Transformer__ architecture that targets high accuracy at the cost of greater model complexity and resource consumption\n",
    "* __Deep Averaging Network(DAN)__ that targets efficient inference with slightly reduced accuracy using simple architecture\n",
    "\n",
    "Both models were trained with the __Stanford Natural Language Inference (SNLI)__ corpus. The [SNLI](https://nlp.stanford.edu/pubs/snli_paper.pdf) corpus is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as __recognizing textual entailment (RTE)__. Essentially, the models were trained to learn the semantic similarity between the sentence pairs.\n",
    "\n",
    "__This model is trained using DAN.__ DAN works in three simple steps:\n",
    "1. take the vector average of the embeddings associated with an input sequence of tokens\n",
    "2. pass that average through one or more feedforward layers\n",
    "3. perform (linear) classification on the final layer’s representation\n",
    "\n",
    "![](./source/dan.png)\n",
    "\n",
    "The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence.\n",
    "\n",
    "This module is about 1GB. Depending on your network speed, it might take a while to load the first time you run inference with it. After that, loading the model should be faster as modules are cached by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:37:59.001576Z",
     "start_time": "2020-06-09T13:32:17.616570Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can change this model. check the url 'https://tfhub.dev/google/' for more\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
    "display(Markdown(\"__Embedding size__: {}\".format(embed_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the Universal Sentence Encoder and computing the embeddings for some text can be as easy as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:38:08.641537Z",
     "start_time": "2020-06-09T13:37:59.003354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "             \"There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join((str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation\n",
    "\n",
    "We have loaded the Universal Sentence Encoder as variable `embed`. To have it work with Keras, it is necessary to wrap it in a Keras Lambda layer and explicitly cast its input as a string. Then we build the Keras model in its standard Functional API. We can view the model summary and realize that __only the Keras layers are trainable, that is how the transfer learning task works by assuring the Universal Sentence Encoder weights untouched__.\n",
    "\n",
    "Now, the let's eliminate the confusion between the terms that is used in deep learning aspect - __loss function__ and __optimizer__.\n",
    "\n",
    "The __loss function__ is a mathematical way of measuring how wrong the predictions are.\n",
    "\n",
    "During the training process, we tweak and change the parameters (weights) of the model to __try and minimize that loss function__, and make the predictions as correct and optimized as possible. But how exactly is it done, by how much, and when?\n",
    "\n",
    "_This is where optimizers come in_. They tie together the loss function and model parameters by updating the model in response to the output of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:45:12.917786Z",
     "start_time": "2020-06-09T13:45:11.570310Z"
    }
   },
   "outputs": [],
   "source": [
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)), \n",
    "                 signature=\"default\", as_dict=True)[\"default\"]\n",
    "\n",
    "input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)\n",
    "\n",
    "# experiment on the custom FC layer here\n",
    "#------------------------------------------------------#\n",
    "x = layers.Dense(256, activation='relu')(embedding)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.125)(x)\n",
    "x = layers.Dense(category_counts, activation='sigmoid')(x)\n",
    "#------------------------------------------------------#\n",
    "\n",
    "model_sa = Model(inputs=[input_text], outputs=x)\n",
    "\n",
    "# we are selecting Adam optimizer - one of the best optimizer in this field\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# setting `binary_crossentropy` as loss function for the classifier\n",
    "model_sa.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_sa.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now, we train the model with the training dataset and validate its performance at the end of each training epoch with validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T13:58:08.136753Z",
     "start_time": "2020-06-09T13:45:29.382567Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model_sa.fit(train_text, train_label,\n",
    "                            validation_data=(val_text, val_label),\n",
    "                            epochs=5,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True)\n",
    "    model_sa.save_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:07:31.738288Z",
     "start_time": "2020-06-09T14:03:59.136968Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    model_sa.load_weights('best_model.h5')\n",
    "    _, train_acc = model_sa.evaluate(train_text, train_label)\n",
    "    _, test_acc = model_sa.evaluate(test_text, test_label)\n",
    "\n",
    "clear_output()\n",
    "display(Markdown('__Train Accuracy__: {}, __Test Accuracy__: {}'.format(round(train_acc,4), round(test_acc,4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Learning Curve of loss and accuracy of the model on each epoch are shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:10:48.185483Z",
     "start_time": "2020-06-09T14:10:47.476017Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(50)), y=history.history['accuracy'], name='train'),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=list(range(50)), y=history.history['val_accuracy'], name='validation'),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(50)), y=history.history['loss'], name='train'),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=list(range(50)), y=history.history['val_loss'], name='validation'),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=600, width=900, showlegend=False,hovermode=\"x\",\n",
    "                  title_text=\"Train and Validation Accuracy and Loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Finally, lets perform some predictions to see where and why are we getting false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T11:22:32.371378Z",
     "start_time": "2020-06-10T11:21:25.947314Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    model_sa.load_weights('best_model.h5')\n",
    "    predicts = model_sa.predict(test_text, batch_size=32)\n",
    "\n",
    "categories = train_df['target'].unique().tolist()\n",
    "\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "test_df['predicted'] = [categories[i] for i in predict_logits]\n",
    "\n",
    "def highlight_rows(x):\n",
    "    if x['target'] != x['predicted']:\n",
    "        return ['background-color: #d65f5f']*3\n",
    "    else:\n",
    "        return ['background-color: lightgreen']*3\n",
    "\n",
    "clear_output()\n",
    "display(test_df.head(20).style.apply(highlight_rows, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform prediction on the clean test data set provided along with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:14:02.005791Z",
     "start_time": "2020-06-09T14:14:01.981536Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names =  ['target', 'id', 'date', 'flag','user','text']\n",
    "\n",
    "test_df = pd.read_csv('./data/sentiment140data/test_data.csv', encoding = \"ISO-8859-1\", names=col_names)\n",
    "\n",
    "print(colored('TEST DATA','magenta',attrs=['bold']))\n",
    "display(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:14:03.147007Z",
     "start_time": "2020-06-09T14:14:03.134867Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "test_df.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)\n",
    "\n",
    "# replacing positive sentiment 4 with 1\n",
    "test_df['target'] = test_df['target'].replace(4,1)\n",
    "\n",
    "target_count = test_df['target'].value_counts()\n",
    "\n",
    "category_counts = len(target_count)\n",
    "display(Markdown('__Number of categories__: {}'.format(category_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are witnessing 3 categories instead of 2, the extra sentiment that we have is __neutral__, but we haven't trained the model on it. Even if we try to keep it, during one-hot encoding, we will obtain 3 columns which will go against the model output architecture (which has 2). So we have to discard this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:14:04.714122Z",
     "start_time": "2020-06-09T14:14:04.705541Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['target'].isin([0, 1])]\n",
    "target_count = test_df['target'].value_counts()\n",
    "\n",
    "category_counts = len(target_count)\n",
    "display(Markdown('__Number of categories__: {}'.format(category_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will evaluate and predict the data __with and without all the text preprocessing__, and analyze the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:14:07.440487Z",
     "start_time": "2020-06-09T14:14:07.292122Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['processed_text'] = test_df['text'].apply(lambda x: preprocess(x))\n",
    "\n",
    "new_test_text = test_df['text'].tolist()\n",
    "new_test_text_p = test_df['processed_text'].tolist()\n",
    "\n",
    "new_test_text = np.array(new_test_text, dtype=object)[:, np.newaxis]\n",
    "new_test_text_p = np.array(new_test_text_p, dtype=object)[:, np.newaxis]\n",
    "\n",
    "new_test_label = np.asarray(pd.get_dummies(test_df['target']), dtype = np.int8)\n",
    "\n",
    "display(Markdown('__New Test Data size__: {}'.format(new_test_label.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:14:25.466118Z",
     "start_time": "2020-06-09T14:14:14.770640Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    \n",
    "    model_sa.load_weights('best_model.h5')\n",
    "    \n",
    "    new_predicts = model_sa.predict(new_test_text, batch_size=16)\n",
    "    new_predicts_p = model_sa.predict(new_test_text_p, batch_size=16)\n",
    "    \n",
    "    _,new_acc = model_sa.evaluate(new_test_text, new_test_label)\n",
    "    _,new_acc_p = model_sa.evaluate(new_test_text_p, new_test_label)\n",
    "    \n",
    "clear_output()\n",
    "display(Markdown('__New test data evaluation__'))\n",
    "display(Markdown('__Without preprocessing__: {} || __With preprocessing__: {}'.format(round(new_acc,4), \n",
    "                                                                                      round(new_acc_p,4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a significant difference. Let's try to see some of the outputs to understand where is the difference coming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T10:56:34.584165Z",
     "start_time": "2020-06-10T10:56:34.542003Z"
    }
   },
   "outputs": [],
   "source": [
    "new_categories = test_df['target'].unique().tolist()\n",
    "\n",
    "predict_logits = new_predicts.argmax(axis=1)\n",
    "test_df['predicted'] = [categories[i] for i in predict_logits]\n",
    "\n",
    "predict_logits = new_predicts_p.argmax(axis=1)\n",
    "test_df['processed_predicted'] = [categories[i] for i in predict_logits]\n",
    "\n",
    "def highlight_rows(x):\n",
    "    if x['target'] != x['predicted'] or x['target'] != x['processed_predicted']:\n",
    "        return ['background-color: #d65f5f']*5\n",
    "    else:\n",
    "        return ['background-color: lightgreen']*5\n",
    "\n",
    "display(test_df.head(20).style.apply(highlight_rows, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "The objective of this notebook is to analyze and classify the sentiment of Tweets obtained from Twitter as positive or negative.\n",
    "\n",
    "After identifying the relevant columns required, we performed an intensive text preprocessing that can be provided as an input to the model, without the need to even tokenization - a step that is required in traditional deep learning approach.\n",
    "\n",
    "Using Universal Sentence Encoder, which is a state-of-the-art pre-trained sentence embedding module, we contextualized the tweets and created a model that holds the information as to which tweets are referring to a positive sentiment, and which ones are negative.\n",
    "\n",
    "An interesting observation is that despite the architecture on which this Encoder works yields less accuracy, the dataset not properly tagged, and such a small number of NN layers - we are obtaining a pretty decent accuracy."
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1591463988754,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

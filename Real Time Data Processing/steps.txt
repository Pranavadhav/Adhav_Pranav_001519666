steps:
    create docker compose file:
      In that write code to install :
        kafka 3.3 
        zookeeper 3.8 (To manage kafka server )(for above kafka 3.4 no need of zookeeper its inbuild)
        Spark master 3.3 (for manage spark worker)
        spark worker 3.3 (To run actual process)
        you can use local python but in my case it didnt work so install on docker
    run docker compose : docker-compose up !!! to close :docker-compose down

    After running all the container 
    1. Start new terminal and lauch kafka producer in that 
      1.do docker ps and get id of the container 
      2. docker exec -it 0d3862361273 bash replace with that id and use command to get into it 
      3. after getting into that  kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test_topic 

    2. Start new terminal and lauch kafka consumer in that and see if data your are genrating are coming or not 
    same producer but kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test_topic

    if you are generating data from python
      write docker file to give libraries you want in your code 
      write .py file where ypu streaming data (use confluent kafka to connect with your server)
      copy your .py file and data into python container docker cp real_data.py f68dda84915f:/
      open terminal to get into it docker exec -it f68dda84915f bash 
      run your python file that you copied using python real_data.py
    
    3.write another .py with light transforamtion
    run it with spark-submit (it will use spark to process that paralley)
    add libraries in spark submit to run 
    spark-submit --packages org.apache.hadoop:hadoop-aws:2.7.1 --conf spark.hadoop.fs.s3a.access.key=access_key --conf spark.hadoop.fs.s3a.secret.key=secret_key --conf "spark.pyspark.python=python" test_stream.py
    it load your data to amazon s3


